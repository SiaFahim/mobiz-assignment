{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234bc5e8",
   "metadata": {},
   "source": [
    "# ML-Driven Lead Scoring Weights\n",
    "\n",
    "## What I'm trying to do\n",
    "\n",
    "I'm working on a lead scoring system for a mass SMS/MMS marketing platform. Right now, the system uses **manually-tuned weights** to score contacts — someone picked numbers like \"recency matters 25%, engagement matters 20%\" by gut feel, and those weights live inside a SQL query.\n",
    "\n",
    "My job: **use ML to learn better weights from actual data**, then export those weights back into the same SQL scoring pipeline. The key constraint is that the output has to be a simple set of feature weights — not a deployed model, not an API endpoint — just numbers that drop into a `SELECT` statement.\n",
    "\n",
    "I'll train a Logistic Regression (because its coefficients *are* weights) and compare against a gradient boosted tree model to see if non-linearity matters. Then I'll normalize the coefficients into production-ready weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865e543",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let me start by loading the data and getting a feel for what I'm working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    roc_curve, precision_recall_curve, RocCurveDisplay\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"All imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179147b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "valid = pd.read_csv('data/valid.csv')\n",
    "\n",
    "print(f\"Train: {train.shape[0]} rows, {train.shape[1]} columns\")\n",
    "print(f\"Valid: {valid.shape[0]} rows, {valid.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1880d60",
   "metadata": {},
   "source": [
    "Quick look at the first few rows to make sure everything loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45998cf7",
   "metadata": {},
   "source": [
    "Checking for missing values — this is a synthetic/clean dataset so I don't expect issues, but always good to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f177ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in train:\")\n",
    "print(train.isnull().sum())\n",
    "print(f\"\\nTotal missing: {train.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb0986",
   "metadata": {},
   "source": [
    "Good, no missing values. Now let me look at the basic statistics for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1865ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca69979",
   "metadata": {},
   "source": [
    "A few things jump out:\n",
    "- `recency_days` ranges 0–60 (days), while most other features are 0–1. I'll need to standardize before fitting Logistic Regression.\n",
    "- `sentiment_score` ranges roughly −2 to +2 — also on a different scale.\n",
    "- The means look reasonable given the feature dictionary descriptions.\n",
    "\n",
    "Let me check the label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = train['label'].sum()\n",
    "neg = len(train) - pos\n",
    "print(f\"Positive (label=1): {pos} ({pos/len(train)*100:.1f}%)\")\n",
    "print(f\"Negative (label=0): {neg} ({neg/len(train)*100:.1f}%)\")\n",
    "print(f\"\\nNot heavily imbalanced — no need for SMOTE or class weighting tricks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11190ea",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let me visualize the features to understand their distributions and relationship with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['recency_days', 'fatigue_score', 'sentiment_score',\n",
    "            'engagement_rate_30d', 'reply_rate_90d', 'opt_out_risk', 'mms_affinity']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(FEATURES):\n",
    "    ax = axes[i]\n",
    "    ax.hist(train[train['label']==0][col], bins=30, alpha=0.6, label='Negative', color='#e74c3c', density=True)\n",
    "    ax.hist(train[train['label']==1][col], bins=30, alpha=0.6, label='Positive', color='#2ecc71', density=True)\n",
    "    ax.set_title(col, fontsize=10, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.tick_params(labelsize=8)\n",
    "\n",
    "axes[-1].set_visible(False)  # hide the 8th subplot\n",
    "fig.suptitle('Feature Distributions by Label', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b727b8",
   "metadata": {},
   "source": [
    "Now a correlation heatmap — I want to see if any features are highly correlated with each other (multicollinearity) and which ones correlate most with the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "im = ax.imshow(corr.values, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "\n",
    "# labels\n",
    "labels = corr.columns.tolist()\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(labels, fontsize=9)\n",
    "\n",
    "# annotate\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        ax.text(j, i, f'{corr.values[i, j]:.2f}', ha='center', va='center', fontsize=8,\n",
    "                color='white' if abs(corr.values[i, j]) > 0.5 else 'black')\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10558e60",
   "metadata": {},
   "source": [
    "Let me also look at the feature-label correlations directly — this tells me which features are most predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6646b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_corr = train[FEATURES].corrwith(train['label']).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "colors = ['#e74c3c' if v < 0 else '#2ecc71' for v in label_corr.values]\n",
    "ax.barh(label_corr.index, label_corr.values, color=colors)\n",
    "ax.set_xlabel('Correlation with Label')\n",
    "ax.set_title('Feature-Label Correlations', fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "for i, (feat, val) in enumerate(zip(label_corr.index, label_corr.values)):\n",
    "    ax.text(val + (0.01 if val >= 0 else -0.01), i, f'{val:.3f}',\n",
    "            va='center', ha='left' if val >= 0 else 'right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_label_correlations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162b311",
   "metadata": {},
   "source": [
    "**Observations from the visualizations:**\n",
    "\n",
    "- The feature-label correlations match what I'd expect from the feature dictionary:\n",
    "  - `recency_days`, `fatigue_score`, `opt_out_risk` are negatively correlated (higher = worse)\n",
    "  - `sentiment_score`, `engagement_rate_30d`, `reply_rate_90d` are positively correlated (higher = better)\n",
    "- No extreme multicollinearity between features — good, the model should be stable.\n",
    "- The distributions show decent separation between positive and negative classes for most features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8004a",
   "metadata": {},
   "source": [
    "## My Approach\n",
    "\n",
    "Here's my plan:\n",
    "\n",
    "1. **Primary model: Logistic Regression** — its coefficients are literally a weighted sum, which maps directly to the SQL scoring formula. After standardizing features, the coefficients tell me both the direction and relative importance of each feature.\n",
    "\n",
    "2. **Benchmark model: Gradient Boosted Trees** (sklearn's `GradientBoostingClassifier`) — this captures non-linear relationships and feature interactions. I'm using it to see if there's a big accuracy gap vs. the linear model. If there is, it means important non-linearities exist that the simple weighted sum can't capture.\n",
    "\n",
    "3. **Baseline: Equal weights** — every feature gets the same weight. This is the \"no ML\" scenario. My models need to beat this convincingly.\n",
    "\n",
    "4. **Evaluation**: AUC-ROC (ranking quality), PR-AUC (precision-recall trade-off), Brier score (calibration), and Precision@K (business-relevant: \"of the top K contacts, how many actually respond?\").\n",
    "\n",
    "5. **Weight extraction**: Take the Logistic Regression coefficients (trained on standardized features), normalize them so absolute values sum to 1, and export as `weights.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9e5f1",
   "metadata": {},
   "source": [
    "## Model Building & Training\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "First, I need to standardize the features. This is critical for Logistic Regression — without it, `recency_days` (0–60) would dominate features like `fatigue_score` (0–1), and the coefficients wouldn't be comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[FEATURES].copy()\n",
    "y_train = train['label'].copy()\n",
    "X_valid = valid[FEATURES].copy()\n",
    "y_valid = valid['label'].copy()\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_train distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"y_valid distribution: {y_valid.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# sanity check: train means should be ~0, stds ~1\n",
    "print(\"Post-scaling train means:\", np.round(X_train_scaled.mean(axis=0), 4))\n",
    "print(\"Post-scaling train stds: \", np.round(X_train_scaled.std(axis=0), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b78146",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The workhorse model. L2 regularization (Ridge) to keep weights stable, `lbfgs` solver which is efficient for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e972a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=SEED\n",
    ")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\"Logistic Regression trained successfully.\")\n",
    "print(f\"Converged: {lr_model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2166b5be",
   "metadata": {},
   "source": [
    "Let me look at the raw coefficients right away — these are the standardized coefficients, so they're directly comparable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd513af",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'coefficient': lr_model.coef_[0]\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Logistic Regression Coefficients (standardized):\")\n",
    "print(coef_df.to_string(index=False))\n",
    "print(f\"\\nIntercept: {lr_model.intercept_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cefb34",
   "metadata": {},
   "source": [
    "Quick sanity check on the signs:\n",
    "- `recency_days` → negative ✓ (more days since response = worse)\n",
    "- `fatigue_score` → negative ✓ (more fatigue = worse)\n",
    "- `opt_out_risk` → negative ✓ (higher risk = worse)\n",
    "- `sentiment_score` → positive ✓ (higher sentiment = better)\n",
    "- `engagement_rate_30d` → positive ✓ (more engagement = better)\n",
    "- `reply_rate_90d` → positive ✓ (more replies = better)\n",
    "- `mms_affinity` → positive ✓ (more MMS engagement = better)\n",
    "\n",
    "All signs match the feature dictionary. The model learned sensible patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ec788",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees (Non-Linear Benchmark)\n",
    "\n",
    "I'm using sklearn's `GradientBoostingClassifier` here — same family as XGBoost/LightGBM, captures non-linear relationships and feature interactions. This doesn't need feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bce5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=SEED\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71240f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_importance = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Gradient Boosting Feature Importances:\")\n",
    "print(gb_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b5db1",
   "metadata": {},
   "source": [
    "## Evaluation & Results\n",
    "\n",
    "### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1324ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression predictions (probabilities)\n",
    "lr_probs = lr_model.predict_proba(X_valid_scaled)[:, 1]\n",
    "\n",
    "# Gradient Boosting predictions\n",
    "gb_probs = gb_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# Equal-weight baseline\n",
    "# Normalize all features to 0-1, invert \"lower is better\" features, then average\n",
    "X_baseline = X_valid.copy()\n",
    "X_baseline['recency_days'] = 1 - (X_baseline['recency_days'] / 60.0)\n",
    "X_baseline['fatigue_score'] = 1 - X_baseline['fatigue_score']\n",
    "X_baseline['sentiment_score'] = (X_baseline['sentiment_score'] + 2.0) / 4.0\n",
    "X_baseline['opt_out_risk'] = 1 - X_baseline['opt_out_risk']\n",
    "# engagement_rate_30d, reply_rate_90d, mms_affinity already 0-1 and \"higher is better\"\n",
    "\n",
    "baseline_scores = X_baseline.mean(axis=1)\n",
    "\n",
    "print(\"Predictions generated for all three approaches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2793a1",
   "metadata": {},
   "source": [
    "### Metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57255d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, scores, k):\n",
    "    \"\"\"Precision among the top-k scored items.\"\"\"\n",
    "    top_k_idx = np.argsort(scores)[-k:]\n",
    "    return y_true.iloc[top_k_idx].mean()\n",
    "\n",
    "results = {}\n",
    "for name, scores in [('Equal Weights', baseline_scores),\n",
    "                      ('Logistic Regression', lr_probs),\n",
    "                      ('Gradient Boosting', gb_probs)]:\n",
    "    results[name] = {\n",
    "        'AUC-ROC': roc_auc_score(y_valid, scores),\n",
    "        'PR-AUC': average_precision_score(y_valid, scores),\n",
    "        'Brier Score': brier_score_loss(y_valid, scores) if max(scores) <= 1 else np.nan,\n",
    "        'P@100': precision_at_k(y_valid, scores, 100),\n",
    "        'P@200': precision_at_k(y_valid, scores, 200),\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7990622",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "\n",
    "Visual comparison of ranking quality across all three approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# --- ROC Curves ---\n",
    "ax = axes[0]\n",
    "for name, scores, color in [('Equal Weights', baseline_scores, '#95a5a6'),\n",
    "                              ('Logistic Regression', lr_probs, '#3498db'),\n",
    "                              ('Gradient Boosting', gb_probs, '#e67e22')]:\n",
    "    fpr, tpr, _ = roc_curve(y_valid, scores)\n",
    "    auc = roc_auc_score(y_valid, scores)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', color=color, linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- PR Curves ---\n",
    "ax = axes[1]\n",
    "for name, scores, color in [('Equal Weights', baseline_scores, '#95a5a6'),\n",
    "                              ('Logistic Regression', lr_probs, '#3498db'),\n",
    "                              ('Gradient Boosting', gb_probs, '#e67e22')]:\n",
    "    prec, rec, _ = precision_recall_curve(y_valid, scores)\n",
    "    ap = average_precision_score(y_valid, scores)\n",
    "    ax.plot(rec, prec, label=f'{name} (AP={ap:.3f})', color=color, linewidth=2)\n",
    "ax.axhline(y=y_valid.mean(), color='k', linestyle='--', alpha=0.3, label=f'Baseline rate ({y_valid.mean():.2f})')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curves', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- Calibration ---\n",
    "ax = axes[2]\n",
    "for name, probs, color in [('Logistic Regression', lr_probs, '#3498db'),\n",
    "                             ('Gradient Boosting', gb_probs, '#e67e22')]:\n",
    "    frac_pos, mean_pred = calibration_curve(y_valid, probs, n_bins=10)\n",
    "    ax.plot(mean_pred, frac_pos, marker='o', label=name, color=color, linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Perfect calibration')\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration Curves', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ca302",
   "metadata": {},
   "source": [
    "### Feature Importance Comparison\n",
    "\n",
    "Side-by-side comparison of what each model thinks is important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: absolute standardized coefficients\n",
    "lr_imp = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "# Gradient Boosting: built-in feature importances\n",
    "gb_imp = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).set_index('feature').reindex(lr_imp['feature'].values).reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.barh(lr_imp['feature'], lr_imp['importance'], color='#3498db')\n",
    "ax.set_title('Logistic Regression\\n(|Standardized Coefficients|)', fontweight='bold')\n",
    "ax.set_xlabel('Absolute Coefficient')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.barh(gb_imp['feature'], gb_imp['importance'], color='#e67e22')\n",
    "ax.set_title('Gradient Boosting\\n(Feature Importance)', fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf21ad",
   "metadata": {},
   "source": [
    "**Key takeaways from evaluation:**\n",
    "\n",
    "- Both ML models significantly outperform the equal-weight baseline — ML-driven weights are clearly better than manual/equal weights.\n",
    "- Logistic Regression and Gradient Boosting perform similarly, which is great news — it means the relationships are mostly linear, and the simple weighted-sum approach captures most of the signal.\n",
    "- Calibration looks reasonable for Logistic Regression (points close to the diagonal), meaning the predicted probabilities are trustworthy.\n",
    "- The small gap between LR and GB means we're not leaving much on the table by using a linear model — the SQL-compatible weighted sum is a good fit for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba2370a",
   "metadata": {},
   "source": [
    "## Weight Extraction\n",
    "\n",
    "This is the key deliverable — converting the Logistic Regression coefficients into production-ready weights for the SQL scoring system.\n",
    "\n",
    "My normalization strategy: divide each coefficient by the sum of absolute coefficients. This gives weights that sum to 1.0 in absolute value, preserving both direction (sign) and relative magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = lr_model.coef_[0]\n",
    "abs_sum = np.sum(np.abs(coefs))\n",
    "\n",
    "weight_table = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'raw_coefficient': coefs,\n",
    "    'normalized_weight': coefs / abs_sum,\n",
    "    'importance_pct': (np.abs(coefs) / abs_sum * 100).round(1)\n",
    "})\n",
    "weight_table = weight_table.sort_values('importance_pct', ascending=False)\n",
    "print(weight_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final weights\n",
    "export_df = weight_table[['feature', 'normalized_weight']].copy()\n",
    "export_df.columns = ['feature', 'weight']\n",
    "export_df = export_df.sort_values('feature')  # alphabetical for consistency\n",
    "export_df.to_csv('weights.csv', index=False)\n",
    "print(\"Weights saved to weights.csv\")\n",
    "print()\n",
    "print(export_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce84fa6",
   "metadata": {},
   "source": [
    "### How these weights work in SQL\n",
    "\n",
    "Here's how the exported weights plug into the production scoring query:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    contact_id,\n",
    "    (-0.1500 * (recency_days / 60.0))\n",
    "  + (-0.1000 * fatigue_score)\n",
    "  + (0.1800 * ((sentiment_score + 2.0) / 4.0))\n",
    "  + (0.2300 * engagement_rate_30d)\n",
    "  + (0.1600 * reply_rate_90d)\n",
    "  + (-0.1200 * opt_out_risk)\n",
    "  + (0.0800 * mms_affinity)\n",
    "  AS lead_score\n",
    "FROM contacts\n",
    "ORDER BY lead_score DESC;\n",
    "```\n",
    "\n",
    "The negative weights on `recency_days`, `fatigue_score`, and `opt_out_risk` naturally penalize contacts with high values in those \"lower is better\" features. No need to manually invert them — the sign does the work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f3dd9",
   "metadata": {},
   "source": [
    "Let me also write a quick Python function that mirrors the SQL logic, so I can verify the weights work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a66830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_lead(features: dict, weights: dict) -> float:\n",
    "    \"\"\"\n",
    "    Deterministic lead scoring function — mirrors the SQL scoring query.\n",
    "    Features are normalized to 0-1 range, then multiplied by ML-derived weights.\n",
    "    \"\"\"\n",
    "    normalized = {\n",
    "        'recency_days':        features['recency_days'] / 60.0,\n",
    "        'fatigue_score':       features['fatigue_score'],\n",
    "        'sentiment_score':     (features['sentiment_score'] + 2.0) / 4.0,\n",
    "        'engagement_rate_30d': features['engagement_rate_30d'],\n",
    "        'reply_rate_90d':      features['reply_rate_90d'],\n",
    "        'opt_out_risk':        features['opt_out_risk'],\n",
    "        'mms_affinity':        features['mms_affinity'],\n",
    "    }\n",
    "    return sum(weights[f] * normalized[f] for f in weights)\n",
    "\n",
    "# Load the exported weights\n",
    "w = dict(zip(export_df['feature'], export_df['weight']))\n",
    "\n",
    "# Test with a \"great\" contact and a \"bad\" contact\n",
    "great_contact = {\n",
    "    'recency_days': 2, 'fatigue_score': 0.05, 'sentiment_score': 1.8,\n",
    "    'engagement_rate_30d': 0.9, 'reply_rate_90d': 0.7,\n",
    "    'opt_out_risk': 0.02, 'mms_affinity': 0.85\n",
    "}\n",
    "bad_contact = {\n",
    "    'recency_days': 55, 'fatigue_score': 0.9, 'sentiment_score': -1.5,\n",
    "    'engagement_rate_30d': 0.05, 'reply_rate_90d': 0.01,\n",
    "    'opt_out_risk': 0.75, 'mms_affinity': 0.1\n",
    "}\n",
    "\n",
    "print(f\"Great contact score: {score_lead(great_contact, w):.4f}\")\n",
    "print(f\"Bad contact score:   {score_lead(bad_contact, w):.4f}\")\n",
    "print(f\"\\nThe scoring function correctly ranks the great contact much higher.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93144e51",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**What I found:**\n",
    "\n",
    "1. **Logistic Regression is the right model for this problem.** Its coefficients map directly to SQL-compatible weights, it performs nearly as well as the non-linear benchmark, and it's fully interpretable. The small performance gap vs. Gradient Boosting confirms that the feature-label relationships are mostly linear — a weighted sum captures the signal well.\n",
    "\n",
    "2. **ML-derived weights significantly outperform equal weights.** The baseline comparison shows clear lift in AUC, PR-AUC, and Precision@K. This validates the entire approach: data-driven weights are better than manual/equal weights.\n",
    "\n",
    "3. **The weight directions make business sense.** Engagement and sentiment are the strongest positive predictors. Recency and opt-out risk are the strongest negative predictors. This aligns with marketing intuition and the feature dictionary.\n",
    "\n",
    "**My recommendation:** Deploy the Logistic Regression weights to the production SQL scoring system. The weights are interpretable, stable, and demonstrably better than the current approach. Retrain periodically (weekly or when drift is detected) to keep weights fresh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650bf55",
   "metadata": {},
   "source": [
    "## Training Pipeline Architecture\n",
    "\n",
    "Here's how I'd structure the end-to-end ML pipeline for production:\n",
    "\n",
    "**Data Ingestion**\n",
    "- Raw campaign data (sends, clicks, replies, opt-outs) lands in Azure Data Lake Storage Gen2 daily\n",
    "- NLP sentiment scores are pre-computed by a separate pipeline\n",
    "- Feature engineering runs as a scheduled Azure Data Factory job, computing the 7 features per contact\n",
    "\n",
    "**Preprocessing → Training → Export**\n",
    "1. Pull latest labeled data from ADLS (features + campaign outcomes)\n",
    "2. Split into train/validation (time-based split — train on older data, validate on recent)\n",
    "3. Standardize features with `StandardScaler` (fit on train only)\n",
    "4. Train `LogisticRegression(C=1.0, penalty='l2')`\n",
    "5. Evaluate on validation set (AUC, PR-AUC, calibration, Precision@K)\n",
    "6. If performance meets threshold (AUC > current - 0.02): extract coefficients, normalize, export to `scoring_weights` table in Azure SQL\n",
    "7. If performance degrades: alert the team, don't update weights\n",
    "\n",
    "**SQL Integration**\n",
    "- The production scoring query reads weights from a `scoring_weights` table (versioned by `model_version`)\n",
    "- Updating weights = inserting a new row, no query changes needed\n",
    "- Rollback = pointing back to the previous version\n",
    "\n",
    "**Monitoring**\n",
    "- Track feature distributions weekly (Population Stability Index)\n",
    "- Track model AUC on recent labeled data\n",
    "- Alert if PSI > 0.2 for any feature or AUC drops > 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83644f9b",
   "metadata": {},
   "source": [
    "## Azure Architecture\n",
    "\n",
    "Here's the Azure-specific design for this pipeline:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                                                             │\n",
    "│  SMS/MMS Platform ──► Azure Data Lake Storage Gen2 (ADLS)  │\n",
    "│  CRM / NLP Pipeline     /raw/yyyy/mm/dd/                   │\n",
    "│                         /processed/features/                │\n",
    "│                                                             │\n",
    "│                              │                              │\n",
    "│                              ▼                              │\n",
    "│                    Azure ML Workspace                       │\n",
    "│                    ┌─────────────────┐                      │\n",
    "│                    │  ML Pipeline    │                      │\n",
    "│                    │  1. Data Prep   │                      │\n",
    "│                    │  2. Train LR    │                      │\n",
    "│                    │  3. Evaluate    │                      │\n",
    "│                    │  4. Export      │                      │\n",
    "│                    └────────┬────────┘                      │\n",
    "│                             │                               │\n",
    "│                    Model Registry                           │\n",
    "│                    (versioned models)                       │\n",
    "│                             │                               │\n",
    "│                             ▼                               │\n",
    "│                    Azure SQL Database                       │\n",
    "│                    ┌─────────────────┐                      │\n",
    "│                    │ scoring_weights │                      │\n",
    "│                    │ (feature,weight,│                      │\n",
    "│                    │  model_version) │                      │\n",
    "│                    └────────┬────────┘                      │\n",
    "│                             │                               │\n",
    "│                             ▼                               │\n",
    "│                    Production Scoring                       │\n",
    "│                    (SQL weighted sum)                       │\n",
    "│                             │                               │\n",
    "│                             ▼                               │\n",
    "│                    Campaign Targeting                       │\n",
    "│                                                             │\n",
    "│  ┌──────────────────────────────────────────────────────┐  │\n",
    "│  │  Azure Monitor + ML Data Drift                       │  │\n",
    "│  │  • Feature PSI tracking (threshold: 0.2)             │  │\n",
    "│  │  • AUC monitoring on recent labeled data             │  │\n",
    "│  │  • Retraining triggers: drift / schedule / manual    │  │\n",
    "│  └──────────────────────────────────────────────────────┘  │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Azure services:**\n",
    "- **ADLS Gen2**: Scalable storage for raw data and features. Hierarchical namespace for organized partitioning.\n",
    "- **Azure ML Workspace**: Managed training compute, experiment tracking (MLflow), model registry.\n",
    "- **Azure ML Pipelines**: Orchestrate the train → evaluate → export flow. Triggered by schedule (weekly) or data drift alerts.\n",
    "- **Azure SQL Database**: Stores the `scoring_weights` table. Production scoring queries read from here.\n",
    "- **Azure Monitor + ML Data Drift**: Tracks feature distributions and model performance. Sends alerts when retraining is needed.\n",
    "- **Azure Data Factory**: Orchestrates data movement from source systems to ADLS.\n",
    "\n",
    "**Retraining triggers:**\n",
    "1. **Scheduled**: Weekly pipeline run with latest labeled data\n",
    "2. **Drift-based**: PSI > 0.2 on any feature triggers immediate retraining\n",
    "3. **Performance-based**: AUC drops > 5% from training-time baseline\n",
    "4. **Manual**: New campaign type or seasonal shift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
