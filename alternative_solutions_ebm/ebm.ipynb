{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91dc1f8",
   "metadata": {},
   "source": [
    "# EBM Experiment: Non-Linear Lead Scoring Weights\n",
    "\n",
    "## Why?\n",
    "The original notebook trained a Logistic Regression and a Gradient Boosting model. LR won, but that doesn't mean the signal is purely linear. It maybe means the GB model was the wrong non-linear model for 3,500 rows (it overfit with aggressive hyperparameters).\n",
    "\n",
    "**Explainable Boosting Machine (EBM)** is a GAM (Generalized Additive Model) from Microsoft's InterpretML library. It learns a *non-linear shape function per feature*, then sums them. Like LR but where each \"weight\" is a curve instead of a single number. It's designed for exactly this: small tabular data where you need both accuracy and explainability.\n",
    "\n",
    "The key insight for deployment: SQL doesn't require linearity, it can do `CASE WHEN` / lookup tables. So we can deploy non-linear per-feature functions while keeping deterministic SQL scoring. No model server needed.\n",
    "\n",
    "**This notebook:**\n",
    "1. Reproduces the original LR (linear baseline)\n",
    "2. Trains an improved Gradient Boosting (fixes overfitting)\n",
    "3. Trains an EBM (non-linear + interpretable)\n",
    "4. Compares all three head-to-head on the same metrics\n",
    "5. Exports each model's weights as separate CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018bc607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    roc_curve, precision_recall_curve,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "FEATURES = ['recency_days', 'fatigue_score', 'sentiment_score',\n",
    "            'engagement_rate_30d', 'reply_rate_90d', 'opt_out_risk', 'mms_affinity']\n",
    "\n",
    "print('All imports loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc781386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3500 rows, 7 features\n",
      "Valid: 1200 rows, 7 features\n",
      "Label distribution (train): {0: 2053, 1: 1447}\n",
      "Positive rate: 41.3%\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "valid = pd.read_csv('../data/valid.csv')\n",
    "\n",
    "X_train = train[FEATURES].copy()\n",
    "y_train = train['label'].copy()\n",
    "X_valid = valid[FEATURES].copy()\n",
    "y_valid = valid['label'].copy()\n",
    "\n",
    "print(f'Train: {X_train.shape[0]} rows, {X_train.shape[1]} features')\n",
    "print(f'Valid: {X_valid.shape[0]} rows, {X_valid.shape[1]} features')\n",
    "print(f'Label distribution (train): {y_train.value_counts().to_dict()}')\n",
    "print(f'Positive rate: {y_train.mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416922e",
   "metadata": {},
   "source": [
    "## Model 1 — Logistic Regression (Linear Baseline)\n",
    "\n",
    "Same setup as the original notebook — this is our linear baseline. StandardScaler + L2-regularized LR with `C=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abe1bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR trained. Coefficients (standardized):\n",
      "            feature  coefficient\n",
      "       recency_days    -0.803719\n",
      "    sentiment_score     0.770165\n",
      "engagement_rate_30d     0.506780\n",
      "       opt_out_risk    -0.298746\n",
      "      fatigue_score    -0.241025\n",
      "       mms_affinity     0.212614\n",
      "     reply_rate_90d     0.198128\n",
      "\n",
      "Intercept: -0.4712\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    C=1.0, penalty='l2', solver='lbfgs', max_iter=1000, random_state=SEED\n",
    ")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_probs = lr_model.predict_proba(X_valid_scaled)[:, 1]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'coefficient': lr_model.coef_[0]\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print('LR trained. Coefficients (standardized):')\n",
    "print(coef_df.to_string(index=False))\n",
    "print(f'\\nIntercept: {lr_model.intercept_[0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef8b67",
   "metadata": {},
   "source": [
    "## Model 2 — Improved Gradient Boosting\n",
    "\n",
    "The original GB overfit — deeper trees + higher learning rate + no leaf regularization on 3,500 rows. These params constrain the model to generalize better:\n",
    "- Shallower trees (`max_depth=3` vs 4)\n",
    "- Slower learning rate (`0.05` vs 0.1)\n",
    "- More estimators (`300` vs 100) to compensate for slower learning\n",
    "- Leaf regularization (`min_samples_leaf=20`, `min_samples_split=30`)\n",
    "- Slightly lower subsample (`0.75` vs 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b49927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved GB trained.\n",
      "Feature importances:\n",
      "            feature  importance\n",
      "    sentiment_score    0.265498\n",
      "       recency_days    0.258045\n",
      "engagement_rate_30d    0.149169\n",
      "       opt_out_risk    0.097322\n",
      "      fatigue_score    0.094382\n",
      "       mms_affinity    0.069646\n",
      "     reply_rate_90d    0.065937\n"
     ]
    }
   ],
   "source": [
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=300, max_depth=3, learning_rate=0.05,\n",
    "    min_samples_leaf=20, subsample=0.75, min_samples_split=30,\n",
    "    random_state=SEED\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_probs = gb_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "print('Improved GB trained.')\n",
    "gb_imp = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print('Feature importances:')\n",
    "print(gb_imp.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f42eab",
   "metadata": {},
   "source": [
    "## Model 3 — Explainable Boosting Machine (EBM)\n",
    "\n",
    "EBM learns a non-linear shape function per feature, then sums them — like LR but each \"weight\" is a curve instead of a single number. It's specifically designed for small tabular datasets.\n",
    "\n",
    "Key settings: `max_bins=64` (binning resolution), `interactions=10` (learn top-10 pairwise interactions out of 21 possible pairs), aggressive early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12547027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBM trained.\n",
      "Number of terms: 17\n",
      "Feature terms: ['recency_days', 'fatigue_score', 'sentiment_score', 'engagement_rate_30d', 'reply_rate_90d', 'opt_out_risk', 'mms_affinity', 'recency_days & sentiment_score', 'recency_days & opt_out_risk', 'fatigue_score & engagement_rate_30d', 'fatigue_score & reply_rate_90d', 'sentiment_score & engagement_rate_30d', 'sentiment_score & reply_rate_90d', 'sentiment_score & opt_out_risk', 'sentiment_score & mms_affinity', 'reply_rate_90d & opt_out_risk', 'reply_rate_90d & mms_affinity']\n",
      "Interaction terms: []\n"
     ]
    }
   ],
   "source": [
    "ebm_model = ExplainableBoostingClassifier(\n",
    "    max_bins=64, interactions=10, learning_rate=0.01,\n",
    "    min_samples_leaf=10, max_rounds=10000,\n",
    "    early_stopping_rounds=100, random_state=SEED\n",
    ")\n",
    "ebm_model.fit(X_train, y_train)\n",
    "ebm_probs = ebm_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "print('EBM trained.')\n",
    "print(f'Number of terms: {len(ebm_model.term_names_)}')\n",
    "print(f'Feature terms: {[t for t in ebm_model.term_names_ if \" x \" not in t]}')\n",
    "print(f'Interaction terms: {[t for t in ebm_model.term_names_ if \" x \" in t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7a35e",
   "metadata": {},
   "source": [
    "## Evaluation — Head-to-Head Comparison\n",
    "\n",
    "Same metrics as the original notebook: AUC-ROC, PR-AUC, Brier Score, Precision@100, Precision@200. Plus the equal-weight baseline for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0721ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     AUC-ROC  PR-AUC  Brier Score  P@100  P@200\n",
      "Equal Weights         0.7964  0.7193       0.2193   0.85  0.815\n",
      "Logistic Regression   0.8244  0.7472       0.1664   0.86  0.835\n",
      "Improved GB           0.8035  0.7145       0.1755   0.86  0.790\n",
      "EBM                   0.8146  0.7355       0.1707   0.86  0.830\n"
     ]
    }
   ],
   "source": [
    "def precision_at_k(y_true, scores, k):\n",
    "    \"\"\"Precision among the top-k scored items.\"\"\"\n",
    "    top_k_idx = np.argsort(scores)[-k:]\n",
    "    return y_true.iloc[top_k_idx].mean()\n",
    "\n",
    "# Equal-weight baseline (same logic as original notebook)\n",
    "X_baseline = X_valid.copy()\n",
    "X_baseline['recency_days'] = 1 - (X_baseline['recency_days'] / 60.0)\n",
    "X_baseline['fatigue_score'] = 1 - X_baseline['fatigue_score']\n",
    "X_baseline['sentiment_score'] = (X_baseline['sentiment_score'] + 2.0) / 4.0\n",
    "X_baseline['opt_out_risk'] = 1 - X_baseline['opt_out_risk']\n",
    "baseline_scores = X_baseline.mean(axis=1)\n",
    "\n",
    "results = {}\n",
    "for name, scores in [('Equal Weights', baseline_scores),\n",
    "                      ('Logistic Regression', lr_probs),\n",
    "                      ('Improved GB', gb_probs),\n",
    "                      ('EBM', ebm_probs)]:\n",
    "    results[name] = {\n",
    "        'AUC-ROC': roc_auc_score(y_valid, scores),\n",
    "        'PR-AUC': average_precision_score(y_valid, scores),\n",
    "        'Brier Score': brier_score_loss(y_valid, scores) if max(scores) <= 1 else np.nan,\n",
    "        'P@100': precision_at_k(y_valid, scores, 100),\n",
    "        'P@200': precision_at_k(y_valid, scores, 200),\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1599e59",
   "metadata": {},
   "source": [
    "### Evaluation Curves\n",
    "\n",
    "ROC, Precision-Recall, and Calibration curves for all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98133e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "COLOR_LR = '#3498db'\n",
    "COLOR_GB = '#e67e22'\n",
    "COLOR_EBM = '#2ecc71'\n",
    "COLOR_EQ = '#95a5a6'\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "for name, scores, color in [('Equal Weights', baseline_scores, COLOR_EQ),\n",
    "                              ('LR', lr_probs, COLOR_LR),\n",
    "                              ('Improved GB', gb_probs, COLOR_GB),\n",
    "                              ('EBM', ebm_probs, COLOR_EBM)]:\n",
    "    fpr, tpr, _ = roc_curve(y_valid, scores)\n",
    "    auc = roc_auc_score(y_valid, scores)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', color=color, linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "ax = axes[1]\n",
    "for name, scores, color in [('Equal Weights', baseline_scores, COLOR_EQ),\n",
    "                              ('LR', lr_probs, COLOR_LR),\n",
    "                              ('Improved GB', gb_probs, COLOR_GB),\n",
    "                              ('EBM', ebm_probs, COLOR_EBM)]:\n",
    "    prec, rec, _ = precision_recall_curve(y_valid, scores)\n",
    "    ap = average_precision_score(y_valid, scores)\n",
    "    ax.plot(rec, prec, label=f'{name} (AP={ap:.3f})', color=color, linewidth=2)\n",
    "ax.axhline(y=y_valid.mean(), color='k', linestyle='--', alpha=0.3, label=f'Baseline ({y_valid.mean():.2f})')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curves', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "ax = axes[2]\n",
    "for name, probs, color in [('LR', lr_probs, COLOR_LR),\n",
    "                             ('Improved GB', gb_probs, COLOR_GB),\n",
    "                             ('EBM', ebm_probs, COLOR_EBM)]:\n",
    "    frac_pos, mean_pred = calibration_curve(y_valid, probs, n_bins=10)\n",
    "    ax.plot(mean_pred, frac_pos, marker='o', label=name, color=color, linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Perfect calibration')\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration Curves', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d89f0b",
   "metadata": {},
   "source": [
    "## EBM Shape Functions\n",
    "\n",
    "Each plot shows the learned non-linear relationship between a single feature and the model's log-odds output. These are the 'weights' — but instead of a single number per feature, you get a curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf4299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ebm_global = ebm_model.explain_global()\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, feat in enumerate(FEATURES):\n",
    "    ax = axes_flat[i]\n",
    "    idx = ebm_model.term_names_.index(feat)\n",
    "    bins = ebm_global.data(idx)['names']\n",
    "    scores = ebm_global.data(idx)['scores']\n",
    "    ax.plot(range(len(scores)), scores, color=COLOR_EBM, linewidth=2)\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.set_title(feat, fontweight='bold')\n",
    "    ax.set_xlabel('Bin index')\n",
    "    ax.set_ylabel('Score contribution')\n",
    "\n",
    "axes_flat[-1].axis('off')\n",
    "plt.suptitle('EBM Shape Functions (log-odds contribution per feature)', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4c01a",
   "metadata": {},
   "source": [
    "## Feature Importance Comparison\n",
    "\n",
    "Three views of feature importance — one per model. LR uses absolute standardized coefficients, GB uses impurity-based importance, EBM uses mean absolute score contribution across bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850b114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# LR importances\n",
    "lr_imp = pd.DataFrame({'feature': FEATURES, 'importance': np.abs(lr_model.coef_[0])}).sort_values('importance')\n",
    "ax = axes[0]\n",
    "ax.barh(lr_imp['feature'], lr_imp['importance'], color=COLOR_LR)\n",
    "ax.set_title('LR: |Standardized Coefficients|', fontweight='bold')\n",
    "ax.set_xlabel('Absolute coefficient')\n",
    "\n",
    "# GB importances\n",
    "gb_imp_sorted = gb_imp.sort_values('importance')\n",
    "ax = axes[1]\n",
    "ax.barh(gb_imp_sorted['feature'], gb_imp_sorted['importance'], color=COLOR_GB)\n",
    "ax.set_title('GB: Feature Importances', fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "\n",
    "# EBM importances\n",
    "ebm_importances = []\n",
    "for feat in FEATURES:\n",
    "    idx = ebm_model.term_names_.index(feat)\n",
    "    scores = ebm_global.data(idx)['scores']\n",
    "    ebm_importances.append(np.mean(np.abs(scores)))\n",
    "ebm_imp = pd.DataFrame({'feature': FEATURES, 'importance': ebm_importances}).sort_values('importance')\n",
    "ax = axes[2]\n",
    "ax.barh(ebm_imp['feature'], ebm_imp['importance'], color=COLOR_EBM)\n",
    "ax.set_title('EBM: Mean |Score Contribution|', fontweight='bold')\n",
    "ax.set_xlabel('Mean absolute contribution')\n",
    "\n",
    "plt.suptitle('Feature Importance Comparison', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a75a9",
   "metadata": {},
   "source": [
    "## Weight Export\n",
    "\n",
    "Export each model's weights as CSV files for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c3133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights_lr.csv\n",
      "Saved weights_gb.csv\n",
      "Saved weights_ebm.csv\n",
      "Saved ebm_shape_functions.csv\n",
      "Shape function table: 433 rows\n"
     ]
    }
   ],
   "source": [
    "# LR weights\n",
    "pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'weight': lr_model.coef_[0]\n",
    "}).to_csv('weights_lr.csv', index=False)\n",
    "print('Saved weights_lr.csv')\n",
    "\n",
    "# GB weights\n",
    "pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).to_csv('weights_gb.csv', index=False)\n",
    "print('Saved weights_gb.csv')\n",
    "\n",
    "# EBM weights (mean absolute contribution)\n",
    "pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'mean_contribution': ebm_importances\n",
    "}).to_csv('weights_ebm.csv', index=False)\n",
    "print('Saved weights_ebm.csv')\n",
    "\n",
    "# EBM shape functions (full bin lookup table)\n",
    "shape_rows = []\n",
    "for feat in FEATURES:\n",
    "    idx = ebm_model.term_names_.index(feat)\n",
    "    data = ebm_global.data(idx)\n",
    "    names = data['names']\n",
    "    scores = data['scores']\n",
    "    for j, score in enumerate(scores):\n",
    "        bin_label = str(names[j]) if j < len(names) else ''\n",
    "        shape_rows.append({'feature': feat, 'bin': bin_label, 'score_contribution': score})\n",
    "\n",
    "pd.DataFrame(shape_rows).to_csv('ebm_shape_functions.csv', index=False)\n",
    "print('Saved ebm_shape_functions.csv')\n",
    "print(f'Shape function table: {len(shape_rows)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322cf31",
   "metadata": {},
   "source": [
    "## SQL Deployment Example\n",
    "\n",
    "EBM shape functions can be deployed as `CASE WHEN` lookup tables in SQL. Here's an example for one feature:\n",
    "\n",
    "```sql\n",
    "-- Example: engagement_rate_30d shape function as SQL CASE WHEN\n",
    "SELECT\n",
    "  customer_id,\n",
    "  CASE\n",
    "    WHEN engagement_rate_30d < 0.1 THEN -0.234\n",
    "    WHEN engagement_rate_30d < 0.2 THEN -0.156\n",
    "    WHEN engagement_rate_30d < 0.3 THEN  0.012\n",
    "    WHEN engagement_rate_30d < 0.4 THEN  0.089\n",
    "    WHEN engagement_rate_30d < 0.5 THEN  0.145\n",
    "    ELSE 0.198\n",
    "  END AS engagement_score\n",
    "  -- + similar CASE WHEN for each other feature\n",
    "  -- + intercept\n",
    "FROM leads;\n",
    "```\n",
    "\n",
    "The actual bin boundaries and scores come from `ebm_shape_functions.csv`. Each feature gets its own `CASE WHEN` block, and the final score is the sum of all contributions plus the intercept. No model server needed — pure SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed3920",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This experiment compared four scoring approaches on the same 3,500-row train / 1,200-row validation split:\n",
    "\n",
    "| Model | AUC-ROC | PR-AUC | Brier Score | P@100 | P@200 |\n",
    "|-------|---------|--------|-------------|-------|-------|\n",
    "| Equal Weights | 0.7964 | 0.7193 | 0.2193 | 0.85 | 0.815 |\n",
    "| **Logistic Regression** | **0.8244** | **0.7472** | **0.1664** | **0.86** | **0.835** |\n",
    "| Improved GB | 0.8035 | 0.7145 | 0.1755 | 0.86 | 0.790 |\n",
    "| EBM | 0.8146 | 0.7355 | 0.1707 | 0.86 | 0.830 |\n",
    "\n",
    "### Key findings\n",
    "\n",
    "1. **LR wins across the board.** Logistic Regression achieves the best AUC-ROC (0.8244), PR-AUC (0.7472), Brier Score (0.1664), and P@200 (0.835). The signal in this dataset is predominantly linear — a single weight per feature captures most of the predictive information.\n",
    "\n",
    "2. **EBM is a solid second.** EBM outperforms the Improved GB on every metric (AUC-ROC 0.8146 vs 0.8035, PR-AUC 0.7355 vs 0.7145) and comes close to LR. The ~1 percentage point AUC gap vs LR suggests mild non-linearities exist but don't add much lift on 3,500 rows.\n",
    "\n",
    "3. **Improved GB underperforms.** Even with anti-overfitting hyperparameters (shallower trees, slower learning rate, leaf regularization), GB still trails both LR and EBM. Tree ensembles need more data to outperform additive models on small tabular datasets.\n",
    "\n",
    "4. **EBM shape functions are revealing.** The shape plots show that `recency_days` and `sentiment_score` have the strongest effects, consistent with LR's largest coefficients. Most shapes are roughly monotonic, confirming the linear signal dominance — but some features (e.g., `engagement_rate_30d`) show subtle non-linear kinks that LR cannot capture.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "**Deploy Logistic Regression.** It's the best-performing model, the simplest to deploy (single weight per feature in SQL), and the easiest to explain to stakeholders. The EBM shape functions are valuable as a diagnostic tool — they confirm the linear assumption holds and reveal where the signal lives — but the ~1% AUC improvement doesn't justify the added SQL complexity of `CASE WHEN` lookup tables.\n",
    "\n",
    "If the dataset grows significantly (10k+ rows), re-running the EBM experiment would be worthwhile — non-linear effects may become more learnable with more data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
